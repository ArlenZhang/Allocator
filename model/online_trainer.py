# -*- coding: utf-8 -*-

"""
@Author: lyzhang
@Date: 2019.1.5
@Description: The main training process of on-line learning RL-based RST-DP.
"""
import torch
import random
from config import *
import torch.nn as nn
import torch.optim as optimizer
from model.rl_model import RLModel
from module.controller import Controller

torch.manual_seed(SEED)
random.seed(SEED)


class Trainer:
    def __init__(self):
        self.model = RLModel()
        self.controller = Controller(self.model)

    @staticmethod
    def reward_comp(state_before, state_after):
        """ 根据状态变化情况设计奖惩机制
        """
        reward = 0
        return reward

    def train_(self):
        """ Learning strategy: online learning
            forward server and consumer states and use double DQN to learn from MSE loss, the loss generated by
            our proposed model according to actual training situation unlike the limited offline learning.
        """
        random.seed(SEED)
        mse_loss = nn.MSELoss()
        optimizer_ = optimizer.Adam(self.model.parameters(), lr=LR, weight_decay=l2_penalty)
        idx_ = 0
        for rl_info in self.controller.process_begin():
            self.model.train()
            state_before, rates_before, action, state_after = rl_info
            reward = self.reward_comp(state_before, state_after)
            data_ = (state_before, rates_before, action, reward, state_after)
            q_pred, q_target = self.model(data_)
            rl_loss = mse_loss(q_pred, q_target)
            optimizer_.zero_grad()
            rl_loss.backward()
            optimizer_.step()
            idx_ += 1
            if idx_ % BATCH_SIZE == 0:
                # We choose to update the target net parameters after each period.
                self.model.update_trg_net()
        # 模型存储
        torch.save(self.model, MODEL_SAVED)
